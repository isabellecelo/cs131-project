# cs131-project: ASL-to-English Translation System

## Team Members
- Isabelle Celo (scelo002)
- Sahana Anand (sanan036)
- Huong Le (hle116)
- Chloe Tang (ctang085)

## Project Overview
We are developing a smart camera system that captures ASL (American Sign Language) gestures in real time and translates them into written and spoken English. The system aims to support communication in the healthcare industry between ASL speakers and non-ASL speakers.

## Key Features
- Real-time ASL gesture recognition using a Logitech camera and NVIDIA Jetson Nano
- Edge computing for processing simple gestures
- Cloud processing via AWS for complex gestures
- Text and voice output
- Voice-to-ASL output using a digital character

## Technologies Used
- Logitech Camera (Video input)
- NVIDIA Jetson Nano (Processes simple gestures locally using TensorFlow models)
- TensorFlow (ML models for gesture recognition)
- AWS (Handles complex gestures, translation, and voice synthesis)
- USB Speaker (Audio output)

## Current Progress
- System design drafted
- Component roles defined
- ML pipeline research is ongoing

## Use Cases
- Assist healthcare workers in communicating with Deaf patients
- Enable inclusive communication in hospitals and clinics
- Educational tool for learning ASL

